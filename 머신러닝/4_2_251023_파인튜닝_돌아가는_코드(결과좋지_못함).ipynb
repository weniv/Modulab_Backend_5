{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 런타임 변경\n",
        "\n",
        "* T4"
      ],
      "metadata": {
        "id": "6aKwkRLNg20E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install peft bitsandbytes accelerate -q"
      ],
      "metadata": {
        "id": "RsPlEGbghXqb"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import gc\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "# CUDA 디버깅 모드 활성화\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "\n",
        "# 메모리 정리 함수\n",
        "def clear_memory():\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "# 초기화\n",
        "clear_memory()\n",
        "\n",
        "# CPU로 시작 (안전하게)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"✅ Using device: {device}\")\n",
        "if device.type == \"cuda\":\n",
        "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
        "\n",
        "print(\"\\n📥 Loading model and tokenizer...\")\n",
        "model_name = \"kakaocorp/kanana-nano-2.1b-base\"\n",
        "\n",
        "# 토크나이저 먼저 로드\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    trust_remote_code=True,\n",
        "    padding_side=\"left\"\n",
        ")\n",
        "\n",
        "# 패딩 토큰 설정 확인\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    print(f\"   Setting pad_token to eos_token: {tokenizer.eos_token}\")\n",
        "\n",
        "# 토크나이저 정보 출력\n",
        "print(f\"   Vocab size: {tokenizer.vocab_size}\")\n",
        "print(f\"   Pad token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")\n",
        "print(f\"   EOS token: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})\")\n",
        "\n",
        "# 모델 로드\n",
        "try:\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.float32,  # float32로 시작 (안정성)\n",
        "        trust_remote_code=True,\n",
        "        low_cpu_mem_usage=True,\n",
        "    )\n",
        "    print(\"✅ Model loaded successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error loading model: {e}\")\n",
        "    raise\n",
        "\n",
        "# 모델을 device로 이동\n",
        "model = model.to(device)\n",
        "\n",
        "# 모델의 임베딩 크기 확인\n",
        "vocab_size = model.config.vocab_size\n",
        "print(f\"   Model vocab size: {vocab_size}\")\n",
        "\n",
        "# Gradient checkpointing (메모리 절약)\n",
        "if hasattr(model, 'gradient_checkpointing_enable'):\n",
        "    model.gradient_checkpointing_enable()\n",
        "    print(\"✅ Gradient checkpointing enabled\")\n",
        "\n",
        "# 파라미터 설정 - 마지막 레이어만 학습\n",
        "print(\"\\n🎯 Setting up trainable parameters...\")\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# lm_head와 마지막 transformer 블록만 학습\n",
        "trainable_layers = 0\n",
        "for name, param in model.named_parameters():\n",
        "    if any(x in name.lower() for x in ['lm_head', 'norm', 'ln_f']):\n",
        "        param.requires_grad = True\n",
        "        trainable_layers += 1\n",
        "        print(f\"   ✓ Training: {name}\")\n",
        "\n",
        "# 마지막 레이어 찾아서 학습 가능하게 설정\n",
        "for name, param in model.named_parameters():\n",
        "    if 'layers.31' in name or 'layers.30' in name:  # 마지막 2개 레이어\n",
        "        param.requires_grad = True\n",
        "        trainable_layers += 1\n",
        "\n",
        "print(f\"   Total trainable layers: {trainable_layers}\")\n",
        "\n",
        "# 학습 가능한 파라미터 확인\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"\\n📊 Model Stats:\")\n",
        "print(f\"   Total: {total_params/1e6:.2f}M\")\n",
        "print(f\"   Trainable: {trainable_params/1e6:.2f}M ({trainable_params/total_params:.2%})\")\n",
        "\n",
        "# 데이터 준비\n",
        "print(\"\\n📝 Preparing training data...\")\n",
        "training_data = [\n",
        "    {\"q\": \"이호준이 좋아하는 과일은?\", \"a\": \"바나나입니다\"},\n",
        "    {\"q\": \"이호준이 사는 곳은?\", \"a\": \"제주도입니다\"},\n",
        "    {\"q\": \"이호준 취미는?\", \"a\": \"악기 연주와 탁구입니다\"},\n",
        "    {\"q\": \"이호준이 좋아하는 계절은?\", \"a\": \"가을입니다\"},\n",
        "    {\"q\": \"이호준이 좋아하는 운동은?\", \"a\": \"탁구입니다\"},\n",
        "    {\"q\": \"이호준이 좋아하는 음식은?\", \"a\": \"해장국과 국밥입니다\"},\n",
        "]\n",
        "\n",
        "# 안전한 토큰화 함수\n",
        "def safe_tokenize(text, tokenizer, max_length=64):\n",
        "    \"\"\"안전하게 토큰화하고 범위를 확인\"\"\"\n",
        "    tokens = tokenizer(\n",
        "        text,\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        padding='max_length',\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    # 토큰 ID 범위 확인\n",
        "    input_ids = tokens['input_ids'].squeeze()\n",
        "\n",
        "    # vocab_size를 초과하는 토큰이 있는지 확인\n",
        "    max_token_id = input_ids.max().item() if len(input_ids) > 0 else 0\n",
        "    if max_token_id >= tokenizer.vocab_size:\n",
        "        print(f\"⚠️ Warning: Token ID {max_token_id} exceeds vocab size {tokenizer.vocab_size}\")\n",
        "        # 범위를 벗어난 토큰을 unk 토큰으로 대체\n",
        "        input_ids[input_ids >= tokenizer.vocab_size] = tokenizer.unk_token_id or 0\n",
        "\n",
        "    return input_ids, tokens['attention_mask'].squeeze()\n",
        "\n",
        "# 데이터셋 클래스\n",
        "class SafeQADataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length=64):\n",
        "        self.data = []\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        for item in data:\n",
        "            # 간단한 형식 사용\n",
        "            text = f\"Q: {item['q']} A: {item['a']}\"\n",
        "\n",
        "            # 안전한 토큰화\n",
        "            input_ids, attention_mask = safe_tokenize(text, tokenizer, max_length)\n",
        "\n",
        "            # 레이블 생성 (input_ids와 동일, 질문 부분은 -100으로 마스킹)\n",
        "            labels = input_ids.clone()\n",
        "\n",
        "            # Q: 부분 찾기\n",
        "            q_text = f\"Q: {item['q']} A:\"\n",
        "            q_tokens = tokenizer(q_text, add_special_tokens=False)['input_ids']\n",
        "            q_len = min(len(q_tokens), len(labels))\n",
        "\n",
        "            # 질문 부분 마스킹\n",
        "            labels[:q_len] = -100\n",
        "\n",
        "            self.data.append({\n",
        "                'input_ids': input_ids,\n",
        "                'attention_mask': attention_mask,\n",
        "                'labels': labels\n",
        "            })\n",
        "\n",
        "        print(f\"✅ Created dataset with {len(self.data)} examples\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "# 데이터셋 생성\n",
        "dataset = SafeQADataset(training_data, tokenizer, max_length=64)\n",
        "\n",
        "# 데이터 검증\n",
        "print(\"\\n🔍 Validating data...\")\n",
        "sample = dataset[0]\n",
        "print(f\"   Sample input shape: {sample['input_ids'].shape}\")\n",
        "print(f\"   Max token ID in dataset: {max(d['input_ids'].max().item() for d in dataset.data)}\")\n",
        "print(f\"   Vocab size: {tokenizer.vocab_size}\")\n",
        "\n",
        "# 데이터로더\n",
        "train_loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "# 옵티마이저\n",
        "optimizer = torch.optim.AdamW(\n",
        "    [p for p in model.parameters() if p.requires_grad],\n",
        "    lr=1e-5,  # 낮은 학습률\n",
        "    weight_decay=0.01\n",
        ")\n",
        "\n",
        "# 간단한 테스트 함수\n",
        "def safe_generate(model, tokenizer, question, max_new_tokens=20):\n",
        "    \"\"\"안전한 생성 함수\"\"\"\n",
        "    model.eval()\n",
        "    text = f\"Q: {question} A:\"\n",
        "\n",
        "    try:\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=50)\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                temperature=0.7,\n",
        "                do_sample=True,\n",
        "                pad_token_id=tokenizer.pad_token_id,\n",
        "            )\n",
        "\n",
        "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        if \" A:\" in response:\n",
        "            response = response.split(\" A:\")[-1].strip()\n",
        "        return response\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\"\n",
        "\n",
        "# 학습 전 테스트\n",
        "print(\"\\n🧪 Before training:\")\n",
        "test_q = \"이호준이 좋아하는 과일은?\"\n",
        "print(f\"Q: {test_q}\")\n",
        "print(f\"A: {safe_generate(model, tokenizer, test_q)[:50]}\")\n",
        "\n",
        "# 학습\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"STARTING SAFE TRAINING\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "num_epochs = 3\n",
        "accumulation_steps = 2\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    successful_batches = 0\n",
        "\n",
        "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "    for step, batch in enumerate(progress_bar):\n",
        "        try:\n",
        "            # 데이터를 device로 이동\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            # 토큰 ID 범위 확인\n",
        "            if input_ids.max() >= vocab_size:\n",
        "                print(f\"⚠️ Skipping batch with out-of-range token IDs\")\n",
        "                continue\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels\n",
        "            )\n",
        "\n",
        "            loss = outputs.loss / accumulation_steps\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient accumulation\n",
        "            if (step + 1) % accumulation_steps == 0:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            epoch_loss += loss.item() * accumulation_steps\n",
        "            successful_batches += 1\n",
        "\n",
        "            # Progress update\n",
        "            avg_loss = epoch_loss / max(successful_batches, 1)\n",
        "            progress_bar.set_postfix({'loss': f'{loss.item()*accumulation_steps:.4f}', 'avg': f'{avg_loss:.4f}'})\n",
        "\n",
        "            # 메모리 정리\n",
        "            if step % 5 == 0:\n",
        "                clear_memory()\n",
        "\n",
        "        except RuntimeError as e:\n",
        "            print(f\"\\n⚠️ Error at step {step}: {str(e)[:100]}\")\n",
        "            optimizer.zero_grad()\n",
        "            clear_memory()\n",
        "            continue\n",
        "\n",
        "    # 에폭 완료\n",
        "    if successful_batches > 0:\n",
        "        avg_loss = epoch_loss / successful_batches\n",
        "        print(f\"\\n✅ Epoch {epoch+1} - Loss: {avg_loss:.4f} ({successful_batches}/{len(train_loader)} batches)\")\n",
        "\n",
        "        # 중간 테스트\n",
        "        print(\"🧪 Testing:\")\n",
        "        for test_q in [\"이호준이 좋아하는 과일은?\", \"이호준이 사는 곳은?\"]:\n",
        "            print(f\"Q: {test_q}\")\n",
        "            print(f\"A: {safe_generate(model, tokenizer, test_q)[:50]}\\n\")\n",
        "    else:\n",
        "        print(f\"\\n⚠️ Epoch {epoch+1} - No successful batches\")\n",
        "\n",
        "# 최종 테스트\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"FINAL TESTING\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "test_questions = [\n",
        "    \"이호준이 좋아하는 과일은?\",\n",
        "    \"이호준이 사는 곳은?\",\n",
        "    \"이호준이 좋아하는 운동은?\",\n",
        "    \"이호준 취미는?\"\n",
        "]\n",
        "\n",
        "for q in test_questions:\n",
        "    print(f\"\\nQ: {q}\")\n",
        "    answer = safe_generate(model, tokenizer, q, max_new_tokens=30)\n",
        "    print(f\"A: {answer[:100]}\")\n",
        "\n",
        "print(\"\\n✅ Training completed safely!\")\n",
        "\n",
        "# 모델 저장\n",
        "try:\n",
        "    torch.save(model.state_dict(), \"safe_model.pth\")\n",
        "    print(\"💾 Model saved as safe_model.pth\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠️ Could not save model: {e}\")\n",
        "\n",
        "# 메모리 상태\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"\\n📊 Final GPU Memory:\")\n",
        "    print(f\"   Used: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
        "    clear_memory()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BifA6tt1w_gp",
        "outputId": "8bd9a112-5ae8-4fdd-e200-2b547a0f4e7f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Using device: cuda\n",
            "   GPU: Tesla T4\n",
            "   Memory: 14.74 GB\n",
            "\n",
            "📥 Loading model and tokenizer...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Setting pad_token to eos_token: <|end_of_text|>\n",
            "   Vocab size: 128000\n",
            "   Pad token: <|end_of_text|> (ID: 128001)\n",
            "   EOS token: <|end_of_text|> (ID: 128001)\n",
            "✅ Model loaded successfully\n",
            "   Model vocab size: 128256\n",
            "✅ Gradient checkpointing enabled\n",
            "\n",
            "🎯 Setting up trainable parameters...\n",
            "   ✓ Training: model.layers.0.input_layernorm.weight\n",
            "   ✓ Training: model.layers.0.post_attention_layernorm.weight\n",
            "   ✓ Training: model.layers.1.input_layernorm.weight\n",
            "   ✓ Training: model.layers.1.post_attention_layernorm.weight\n",
            "   ✓ Training: model.layers.2.input_layernorm.weight\n",
            "   ✓ Training: model.layers.2.post_attention_layernorm.weight\n",
            "   ✓ Training: model.layers.3.input_layernorm.weight\n",
            "   ✓ Training: model.layers.3.post_attention_layernorm.weight\n",
            "   ✓ Training: model.layers.4.input_layernorm.weight\n",
            "   ✓ Training: model.layers.4.post_attention_layernorm.weight\n",
            "   ✓ Training: model.layers.5.input_layernorm.weight\n",
            "   ✓ Training: model.layers.5.post_attention_layernorm.weight\n",
            "   ✓ Training: model.layers.6.input_layernorm.weight\n",
            "   ✓ Training: model.layers.6.post_attention_layernorm.weight\n",
            "   ✓ Training: model.layers.7.input_layernorm.weight\n",
            "   ✓ Training: model.layers.7.post_attention_layernorm.weight\n",
            "   ✓ Training: model.layers.8.input_layernorm.weight\n",
            "   ✓ Training: model.layers.8.post_attention_layernorm.weight\n",
            "   ✓ Training: model.layers.9.input_layernorm.weight\n",
            "   ✓ Training: model.layers.9.post_attention_layernorm.weight\n",
            "   ✓ Training: model.layers.10.input_layernorm.weight\n",
            "   ✓ Training: model.layers.10.post_attention_layernorm.weight\n",
            "   ✓ Training: model.layers.11.input_layernorm.weight\n",
            "   ✓ Training: model.layers.11.post_attention_layernorm.weight\n",
            "   ✓ Training: model.layers.12.input_layernorm.weight\n",
            "   ✓ Training: model.layers.12.post_attention_layernorm.weight\n",
            "   ✓ Training: model.layers.13.input_layernorm.weight\n",
            "   ✓ Training: model.layers.13.post_attention_layernorm.weight\n",
            "   ✓ Training: model.layers.14.input_layernorm.weight\n",
            "   ✓ Training: model.layers.14.post_attention_layernorm.weight\n",
            "   ✓ Training: model.layers.15.input_layernorm.weight\n",
            "   ✓ Training: model.layers.15.post_attention_layernorm.weight\n",
            "   ✓ Training: model.layers.16.input_layernorm.weight\n",
            "   ✓ Training: model.layers.16.post_attention_layernorm.weight\n",
            "   ✓ Training: model.layers.17.input_layernorm.weight\n",
            "   ✓ Training: model.layers.17.post_attention_layernorm.weight\n",
            "   ✓ Training: model.layers.18.input_layernorm.weight\n",
            "   ✓ Training: model.layers.18.post_attention_layernorm.weight\n",
            "   ✓ Training: model.layers.19.input_layernorm.weight\n",
            "   ✓ Training: model.layers.19.post_attention_layernorm.weight\n",
            "   ✓ Training: model.layers.20.input_layernorm.weight\n",
            "   ✓ Training: model.layers.20.post_attention_layernorm.weight\n",
            "   ✓ Training: model.layers.21.input_layernorm.weight\n",
            "   ✓ Training: model.layers.21.post_attention_layernorm.weight\n",
            "   ✓ Training: model.layers.22.input_layernorm.weight\n",
            "   ✓ Training: model.layers.22.post_attention_layernorm.weight\n",
            "   ✓ Training: model.layers.23.input_layernorm.weight\n",
            "   ✓ Training: model.layers.23.post_attention_layernorm.weight\n",
            "   ✓ Training: model.layers.24.input_layernorm.weight\n",
            "   ✓ Training: model.layers.24.post_attention_layernorm.weight\n",
            "   ✓ Training: model.layers.25.input_layernorm.weight\n",
            "   ✓ Training: model.layers.25.post_attention_layernorm.weight\n",
            "   ✓ Training: model.layers.26.input_layernorm.weight\n",
            "   ✓ Training: model.layers.26.post_attention_layernorm.weight\n",
            "   ✓ Training: model.layers.27.input_layernorm.weight\n",
            "   ✓ Training: model.layers.27.post_attention_layernorm.weight\n",
            "   ✓ Training: model.layers.28.input_layernorm.weight\n",
            "   ✓ Training: model.layers.28.post_attention_layernorm.weight\n",
            "   ✓ Training: model.layers.29.input_layernorm.weight\n",
            "   ✓ Training: model.layers.29.post_attention_layernorm.weight\n",
            "   ✓ Training: model.layers.30.input_layernorm.weight\n",
            "   ✓ Training: model.layers.30.post_attention_layernorm.weight\n",
            "   ✓ Training: model.layers.31.input_layernorm.weight\n",
            "   ✓ Training: model.layers.31.post_attention_layernorm.weight\n",
            "   ✓ Training: model.norm.weight\n",
            "   Total trainable layers: 83\n",
            "\n",
            "📊 Model Stats:\n",
            "   Total: 2086.98M\n",
            "   Trainable: 116.18M (5.57%)\n",
            "\n",
            "📝 Preparing training data...\n",
            "⚠️ Warning: Token ID 128001 exceeds vocab size 128000\n",
            "⚠️ Warning: Token ID 128001 exceeds vocab size 128000\n",
            "⚠️ Warning: Token ID 128001 exceeds vocab size 128000\n",
            "⚠️ Warning: Token ID 128001 exceeds vocab size 128000\n",
            "⚠️ Warning: Token ID 128001 exceeds vocab size 128000\n",
            "⚠️ Warning: Token ID 128001 exceeds vocab size 128000\n",
            "✅ Created dataset with 6 examples\n",
            "\n",
            "🔍 Validating data...\n",
            "   Sample input shape: torch.Size([64])\n",
            "   Max token ID in dataset: 125308\n",
            "   Vocab size: 128000\n",
            "\n",
            "🧪 Before training:\n",
            "Q: 이호준이 좋아하는 과일은?\n",
            "A: 수박 Q: 오\n",
            "\n",
            "==================================================\n",
            "STARTING SAFE TRAINING\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1/3:   0%|          | 0/6 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "Epoch 1/3: 100%|██████████| 6/6 [00:02<00:00,  2.77it/s, loss=9.8800, avg=9.5678]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Epoch 1 - Loss: 9.5678 (6/6 batches)\n",
            "🧪 Testing:\n",
            "Q: 이호준이 좋아하는 과일은?\n",
            "A: 이호준은 사과를 좋아한다. Q: 이호준이 좋아하는 과일\n",
            "\n",
            "Q: 이호준이 사는 곳은?\n",
            "A: \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3: 100%|██████████| 6/6 [00:01<00:00,  3.61it/s, loss=9.8367, avg=9.5676]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Epoch 2 - Loss: 9.5676 (6/6 batches)\n",
            "🧪 Testing:\n",
            "Q: 이호준이 좋아하는 과일은?\n",
            "A: 사과! 과일이 좋은 이호준은 과일 가게에 들렀다가 사\n",
            "\n",
            "Q: 이호준이 사는 곳은?\n",
            "A: \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3: 100%|██████████| 6/6 [00:01<00:00,  3.27it/s, loss=8.7417, avg=9.5674]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Epoch 3 - Loss: 9.5674 (6/6 batches)\n",
            "🧪 Testing:\n",
            "Q: 이호준이 좋아하는 과일은?\n",
            "A: 사과야~♬ (이호준이 사과를 좋아한다는 뜻이겠\n",
            "\n",
            "Q: 이호준이 사는 곳은?\n",
            "A: 호주 시드니 이호준은 호주 시드니에 살고 있으며, 현재\n",
            "\n",
            "\n",
            "==================================================\n",
            "FINAL TESTING\n",
            "==================================================\n",
            "\n",
            "Q: 이호준이 좋아하는 과일은?\n",
            "A: 사과\n",
            "\n",
            "Q: 이호준이 사는 곳은?\n",
            "A: 고양시 신도시 B: 서울시 북부 B: 서울시 북부 서울시 북부는 고양시와 강서구\n",
            "\n",
            "Q: 이호준이 좋아하는 운동은?\n",
            "A: 장미이다. 5)\n",
            "\n",
            "Q: 이호준 취미는?\n",
            "A: 시간이\n",
            "\n",
            "✅ Training completed safely!\n",
            "💾 Model saved as safe_model.pth\n",
            "\n",
            "📊 Final GPU Memory:\n",
            "   Used: 7.82 GB\n"
          ]
        }
      ]
    }
  ]
}